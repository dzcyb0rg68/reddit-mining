{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from collections import Counter\n",
    "import sys\n",
    "import re\n",
    "import fileinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_active_subreddit_in_2016():\n",
    "    count = 0\n",
    "    filenames = [\n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-01.gz\",\n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-02.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-05.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-07.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-09.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-11.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-03.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-04.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-06.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-08.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-10.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-12.gz\"\n",
    "    ]\n",
    "    \n",
    "    l = []\n",
    "    for filename in filenames:\n",
    "        for line in gzip.open(filename):\n",
    "            line = line.decode(\"utf-8\")\n",
    "            \n",
    "            line = json.loads(line)\n",
    "            l.append(line['subreddit'])\n",
    "    \n",
    "    c = Counter(l)\n",
    "    return (c,c.most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_active_subreddit_in_june_2019():\n",
    "    count = 0\n",
    "\n",
    "    filenames = [\"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2019-06.gz\"]\n",
    "    \n",
    "    l = []\n",
    "    for filename in filenames:\n",
    "        for line in gzip.open(filename):\n",
    "            line = line.decode(\"utf-8\")\n",
    "            \n",
    "            line = json.loads(line)\n",
    "            l.append(line['subreddit'])\n",
    "    \n",
    "    c = Counter(l)\n",
    "    return (c,c.most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d = most_active_subreddit_in_2016()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(c, key=c.get, reverse=True)\n",
    "sorted(c.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count,dd = most_active_subreddit_in_june_2019()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_subreddit = sorted(count, key=count.get, reverse=True)\n",
    "sorted_subreddit_tuple = sorted(count.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comment_rank_subreddit-1-persent_June2019.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s %s' % x for x in sorted_subreddit_tuple))\n",
    "    \n",
    "with open('comment_rank_subreddit-1-persent_June2019.txt') as fp2:\n",
    "    for i, line, x in zip(range(len(sorted_subreddit)),enumerate(fp2),sorted_subreddit):\n",
    "#         print(i+1, x, count[x])\n",
    "        with open('rank.txt','a') as fp3:\n",
    "            fp3.write('%d %s %s %s'%(i+1, x, count[x], 'https://www.reddit.com/r/'+x+'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "# import gzip\n",
    "import lzma\n",
    "\n",
    "filenames = [\"/l/research/social-media-mining/reddit/comments/RC_2018-01.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-02.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-03.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-04.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-05.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-06.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-07.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-08.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-09.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-10.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-11.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-12.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-01.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-02.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-03.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-04.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-05.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-06.zst\"\n",
    "            ]\n",
    "\n",
    "head = open('hulu.txt', 'w+') #hulu is ranked 3956\n",
    "head.write('%s\\t %s\\t %s\\t %s\\t %s\\t\\n'% ('row number', 'author', 'score', 'date', 'comment'))\n",
    "head.close()\n",
    "\n",
    "head = open('netflix.txt', 'w+') #netflix is ranked 1642\n",
    "head.write('%s\\t %s\\t %s\\t %s\\t %s\\t\\n'% ('row number', 'author', 'score', 'date', 'comment'))\n",
    "head.close()\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "l = 0\n",
    "for filename in filenames:\n",
    "    date = filename.replace('.xz','').replace('.zst','')\n",
    "    if filename.split('.',1)[1] == 'xz':\n",
    "        with lzma.open(filename, mode='rt', encoding='utf-8') as redditfile:\n",
    "#     with gzip.open(filename) as redditfile:\n",
    "            for line in redditfile:\n",
    "                line = json.loads(line)\n",
    "                if line['subreddit'] == 'Hulu':    \n",
    "                    file = open('hulu.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %d\\t %s\\t %s\\t\\n'% (i+1, line['author'].replace('\\n',''), line['score'] , date[-7:], line['body'].replace('\\n','')))\n",
    "                    i += 1\n",
    "                    file.close()\n",
    "                    \n",
    "                if line['subreddit'] == 'netflix':    \n",
    "                    file = open('netflix.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %d\\t %s\\t %s\\t\\n'% (j+1, line['author'].replace('\\n',''), line['score'], date[-7:], line['body'].replace('\\n','')))\n",
    "                    j += 1\n",
    "                    file.close()\n",
    "                    \n",
    "    elif filename.split('.',1)[1] == 'zst':\n",
    "        with open(filename, 'rb') as fh:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            stream_reader = dctx.stream_reader(fh)\n",
    "            text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "            z = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "            \n",
    "            for line in text_stream:\n",
    "                line = json.loads(line.translate(z))\n",
    "                if line['subreddit'] == 'Hulu':\n",
    "                    file = open('hulu.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %d\\t %s\\t %s\\t\\n'% (k+1, line['author'].replace('\\n',''), line['score'], date[-7:], line['body'].replace('\\n','')))\n",
    "                    k += 1\n",
    "                    file.close()\n",
    "                \n",
    "                if line['subreddit'] == 'netflix':    \n",
    "                    file = open('netflix.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %d\\t %s\\t %s\\t\\n'% (l+1, line['author'].replace('\\n',''), line['score'], date[-7:], line['body'].replace('\\n','')))\n",
    "                    l += 1\n",
    "                    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
